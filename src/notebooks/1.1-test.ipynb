{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from typing import Dict, List\n",
    "\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_embeddings(embeddings: np.ndarray, syndrome_ids: List) -> Dict:\n",
    "    \"\"\"\n",
    "    Analyze the embeddings data with advanced statistics\n",
    "    Args:\n",
    "        embeddings (np.ndarray): Matrix of embeddings\n",
    "        syndrome_ids (List): List of syndrome IDs\n",
    "\n",
    "    Returns:\n",
    "        Dict: Dictionary containing embedding statistics\n",
    "    \"\"\"\n",
    "    # Basic statistics\n",
    "    embedding_stats = {\n",
    "        \"embedding_dim\": embeddings.shape[1],\n",
    "        \"mean\": np.mean(embeddings, axis=0),\n",
    "        \"std\": np.std(embeddings, axis=0),\n",
    "        \"min\": np.min(embeddings, axis=0),\n",
    "        \"max\": np.max(embeddings, axis=0),\n",
    "        \"skewness\": stats.skew(embeddings, axis=0),\n",
    "        \"kurtosis\": stats.kurtosis(embeddings, axis=0),\n",
    "    }\n",
    "    pca = PCA()\n",
    "    pca_result = pca.fit_transform(embeddings)\n",
    "    embedding_stats[\"explained_variance_ratio\"] = pca.explained_variance_ratio_\n",
    "    embedding_stats[\"cumulative_variance_ratio\"] = np.cumsum(\n",
    "        pca.explained_variance_ratio_\n",
    "    )\n",
    "\n",
    "    # Calculate within-class and between-class statistics\n",
    "    unique_syndromes = list(set(syndrome_ids))\n",
    "    within_class_distances = []\n",
    "    between_class_distances = []\n",
    "\n",
    "    for syndrome in unique_syndromes:\n",
    "        # Get embeddings for current syndrome\n",
    "        syndrome_mask = np.array(syndrome_ids) == syndrome\n",
    "        syndrome_embeddings = embeddings[syndrome_mask]\n",
    "\n",
    "        # Calculate within-class distances\n",
    "        if len(syndrome_embeddings) > 1:\n",
    "            distances = cosine_similarity(syndrome_embeddings)\n",
    "            # Get upper triangle of distance matrix (excluding diagonal)\n",
    "            distances = distances[np.triu_indices(len(distances), k=1)]\n",
    "            within_class_distances.extend(distances)\n",
    "\n",
    "        # Calculate between-class distances\n",
    "        other_embeddings = embeddings[~syndrome_mask]\n",
    "        if len(syndrome_embeddings) > 0 and len(other_embeddings) > 0:\n",
    "            distances = cosine_similarity(syndrome_embeddings, other_embeddings)\n",
    "            between_class_distances.extend(distances.flatten())\n",
    "\n",
    "    embedding_stats[\"within_class_similarity\"] = {\n",
    "        \"mean\": np.mean(within_class_distances),\n",
    "        \"std\": np.std(within_class_distances),\n",
    "        \"min\": np.min(within_class_distances),\n",
    "        \"max\": np.max(within_class_distances),\n",
    "    }\n",
    "    embedding_stats[\"between_class_similarity\"] = {\n",
    "        \"mean\": np.mean(between_class_distances),\n",
    "        \"std\": np.std(between_class_distances),\n",
    "        \"min\": np.min(between_class_distances),\n",
    "        \"max\": np.max(between_class_distances),\n",
    "    }\n",
    "    return embedding_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_separation_metrics(embeddings: np.ndarray, syndrome_ids: List) -> Dict:\n",
    "    \"\"\"\n",
    "    Calculate metrics that indicate how well the embeddings separate different syndromes\n",
    "    Args:\n",
    "    embeddings (np.ndarray): Matrix of embeddings\n",
    "    syndrome_ids (List): List of syndrome IDs\n",
    "\n",
    "    Returns:\n",
    "        Dict: Dictionary containing separation metrics\n",
    "    \"\"\"\n",
    "    unique_syndromes = list(set(syndrome_ids))\n",
    "    separation_metrics = {}\n",
    "\n",
    "    # Calculate syndrome centroids\n",
    "    centroids = {}\n",
    "    for syndrome in unique_syndromes:\n",
    "        syndrome_mask = np.array(syndrome_ids) == syndrome\n",
    "        syndrome_embeddings = embeddings[syndrome_mask]\n",
    "        centroids[syndrome] = np.mean(syndrome_embeddings, axis=0)\n",
    "\n",
    "    # Calculate inter-centroid distances\n",
    "    centroid_distances = {}\n",
    "    for i, syndrome1 in enumerate(unique_syndromes):\n",
    "        for syndrome2 in unique_syndromes[i + 1 :]:\n",
    "            distance = np.linalg.norm(centroids[syndrome1] - centroids[syndrome2])\n",
    "            centroid_distances[f\"{syndrome1}-{syndrome2}\"] = distance\n",
    "\n",
    "    separation_metrics[\"centroid_distances\"] = centroid_distances\n",
    "    separation_metrics[\"mean_centroid_distance\"] = np.mean(\n",
    "        list(centroid_distances.values())\n",
    "    )\n",
    "    separation_metrics[\"min_centroid_distance\"] = np.min(\n",
    "        list(centroid_distances.values())\n",
    "    )\n",
    "    separation_metrics[\"max_centroid_distance\"] = np.max(\n",
    "        list(centroid_distances.values())\n",
    "    )\n",
    "\n",
    "    return separation_metrics"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "APOLLO-VP8OYYUy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
