{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy import array, cumsum, max, mean, min, ndarray, std, triu_indices, linalg\n",
    "from pandas import DataFrame, Series, merge, read_pickle\n",
    "from scipy.stats import kurtosis, skew\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics.pairwise import cosine_similarity, euclidean_distances\n",
    "\n",
    "data = read_pickle(\"../data/raw/mini_gm_public_v0.1.p\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **DATA PROCESSING**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = DataFrame.from_dict(\n",
    "    [\n",
    "        {\n",
    "            \"syndrome_id\": syndrome_id,\n",
    "            \"subject_id\": subject_id,\n",
    "            \"image_id\": image_id,\n",
    "            \"embedding\": embedding,\n",
    "        }\n",
    "        for syndrome_id, subjects in data.items()\n",
    "        for subject_id, images in subjects.items()\n",
    "        for image_id, embedding in images.items()\n",
    "    ]\n",
    ")\n",
    "df[\"embedding_id\"] = df[\"subject_id\"] + \"_\" + df[\"image_id\"]\n",
    "embeddings = df[[\"embedding_id\", \"embedding\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_exploded = embeddings.explode(\"embedding\")\n",
    "df_exploded[\"embedding_index\"] = df_exploded.groupby(\"embedding_id\").cumcount()\n",
    "df_pivoted = (\n",
    "    df_exploded.pivot(\n",
    "        index=\"embedding_id\", columns=\"embedding_index\", values=\"embedding\"\n",
    "    )\n",
    "    .set_axis([f\"embedding_{i}\" for i in range(320)], axis=1)\n",
    "    .reset_index()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "(df := df.drop(columns=[\"embedding\"])).to_csv(\n",
    "    \"../data/interim/flattened_data.csv\", index=False\n",
    ")\n",
    "df_pivoted.to_csv(\"../data/interim/embeddings.csv\", index=False)\n",
    "merge(df, df_pivoted, on=\"embedding_id\", how=\"left\").to_csv(\n",
    "    \"../data/interim/complete_data.csv\", index=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **DATA ANALYSIS**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    df.isna().sum(),\n",
    "    f\"\\n{'-' * 12}\",\n",
    "    \"\\nDuplicates found:\",\n",
    "    df[[\"syndrome_id\", \"subject_id\", \"image_id\"]].duplicated().sum(),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sujects & Syndromes:\n",
    "\n",
    "- Não vale a pena levar em consideração a qunatidade de imagens pro sujeito, a maioria só possui 1 imagem.\n",
    "- Classes desbalanceadas, com a maior sendo '300000034' e a menor '700018215'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_dataset(\n",
    "    df: DataFrame,\n",
    ") -> tuple[dict[str, int], dict[str, defaultdict]]:\n",
    "    \"\"\"\n",
    "    Calculate dataset df_stats and visualize the distribution of images per syndrome.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame containing the data with columns 'syndrome_id', 'subject_id', and 'image_id'\n",
    "\n",
    "    Returns:\n",
    "        tuple[dict[str, int], dict[str, defaultdict]]: tuple containing 2 Dictionary with dataset statistics\n",
    "    \"\"\"\n",
    "    # Extract columns\n",
    "    syndrome_ids = df[\"syndrome_id\"].tolist()\n",
    "    subject_ids = df[\"subject_id\"].tolist()\n",
    "    image_ids = df[\"image_id\"].tolist()\n",
    "    total_images = len(image_ids)\n",
    "\n",
    "    df_stats = {\n",
    "        \"total_images\": len(image_ids),\n",
    "        \"total_subjects\": len(set(subject_ids)),\n",
    "        \"total_syndromes\": len(set(syndrome_ids)),\n",
    "    }\n",
    "    syndrome_stats = {\n",
    "        \"images_per_syndrome\": defaultdict(int),\n",
    "        \"subjects_per_syndrome\": defaultdict(set),\n",
    "        \"class_balance\": defaultdict(float),\n",
    "        # \"images_per_subject\": defaultdict(int),\n",
    "    }\n",
    "    for syndrome_id in syndrome_ids:\n",
    "        syndrome_stats[\"images_per_syndrome\"][syndrome_id] += 1\n",
    "\n",
    "    syndrome_stats[\"class_balance\"] = {\n",
    "        k: v / total_images for k, v in syndrome_stats[\"images_per_syndrome\"].items()\n",
    "    }\n",
    "    for syndrome_id, subject_id in zip(syndrome_ids, subject_ids):\n",
    "        syndrome_stats[\"subjects_per_syndrome\"][syndrome_id].add(subject_id)\n",
    "\n",
    "    syndrome_stats[\"subjects_per_syndrome\"] = {\n",
    "        k: len(v) for k, v in syndrome_stats[\"subjects_per_syndrome\"].items()\n",
    "    }\n",
    "    print(f\"Number of syndromes: {df_stats['total_syndromes']}\")\n",
    "    print(f\"Number of images: {df_stats['total_images']}\")\n",
    "\n",
    "    images_per_syndrome_series = Series(syndrome_stats[\"images_per_syndrome\"])\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    images_per_syndrome_series.plot(kind=\"bar\")\n",
    "    plt.title(\"Number of Images per Syndrome\")\n",
    "    plt.xlabel(\"Syndrome ID\")\n",
    "    plt.ylabel(\"Number of Images\")\n",
    "    plt.show()\n",
    "\n",
    "    # Calculate imbalance threshold and print syndromes below it\n",
    "    imbalance_threshold = df_stats[\"total_images\"] / df_stats[\"total_syndromes\"]\n",
    "    print(\n",
    "        f\"Syndromes with fewer images than {imbalance_threshold}: \\n\"\n",
    "        f\"{images_per_syndrome_series[images_per_syndrome_series < imbalance_threshold]}\"\n",
    "    )\n",
    "    return df_stats, syndrome_stats\n",
    "\n",
    "\n",
    "statistics, syndrome_stats = analyze_dataset(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# json.dumps(statistics)\n",
    "statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = syndrome_stats[\"images_per_syndrome\"]\n",
    "subjects = syndrome_stats[\"subjects_per_syndrome\"]\n",
    "balances = syndrome_stats[\"class_balance\"]\n",
    "\n",
    "syndromes = DataFrame(\n",
    "    [\n",
    "        {\n",
    "            \"syndrome_id\": syndrome_id,\n",
    "            \"images\": images[syndrome_id],\n",
    "            \"subjects\": subjects[syndrome_id],\n",
    "            \"class_balance\": balances[syndrome_id],\n",
    "        }\n",
    "        for syndrome_id in images\n",
    "    ]\n",
    ").set_index(\"syndrome_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 8))\n",
    "ax = fig.add_subplot(111, projection=\"3d\")\n",
    "\n",
    "# TODO: change this plot to 3D bar from plotly\n",
    "scatter = ax.scatter(\n",
    "    syndromes[\"images\"],\n",
    "    syndromes[\"subjects\"],\n",
    "    syndromes[\"class_balance\"],\n",
    "    c=\"#1f77b4\",\n",
    "    alpha=1,\n",
    "    cmap=\"viridis\",\n",
    ")\n",
    "ax.set_xlabel(\"Images\")\n",
    "ax.set_ylabel(\"Subjects\")\n",
    "ax.set_zlabel(\"Class Balance\")\n",
    "ax.set_title(\"3D Scatter Plot of Syndrome Data\")\n",
    "\n",
    "fig.colorbar(scatter, label=\"Syndrome ID\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_embeddings(embeddings: ndarray, syndrome_ids: list) -> dict:\n",
    "    \"\"\"\n",
    "    Analyze the embeddings data with advanced statistics\n",
    "    Args:\n",
    "        embeddings (ndarray): Matrix of embeddings\n",
    "        syndrome_ids (List): List of syndrome IDs\n",
    "\n",
    "    Returns:\n",
    "        Dict: Dictionary containing embedding statistics\n",
    "    \"\"\"\n",
    "    # Basic statistics\n",
    "    embedding_stats = {\n",
    "        \"embedding_dim\": embeddings.shape[1],\n",
    "        \"mean\": mean(embeddings, axis=0),\n",
    "        \"std\": std(embeddings, axis=0),\n",
    "        \"min\": min(embeddings, axis=0),\n",
    "        \"max\": max(embeddings, axis=0),\n",
    "        \"skewness\": skew(embeddings, axis=0),\n",
    "        \"kurtosis\": kurtosis(embeddings, axis=0),\n",
    "    }\n",
    "    pca = PCA().fit_transform(embeddings)\n",
    "    embedding_stats[\"explained_variance_ratio\"] = pca.explained_variance_ratio_\n",
    "    embedding_stats[\"cumulative_variance_ratio\"] = cumsum(pca.explained_variance_ratio_)\n",
    "\n",
    "    # Calculate within-class and between-class statistics\n",
    "    unique_syndromes = list(set(syndrome_ids))\n",
    "    within_class_distances = []\n",
    "    between_class_distances = []\n",
    "\n",
    "    for syndrome in unique_syndromes:\n",
    "        # Get embeddings for current syndrome\n",
    "        syndrome_mask = array(syndrome_ids) == syndrome\n",
    "        syndrome_embeddings = embeddings[syndrome_mask]\n",
    "\n",
    "        # Calculate within-class distances\n",
    "        if len(syndrome_embeddings) > 1:\n",
    "            distances = cosine_similarity(syndrome_embeddings)\n",
    "            # Get upper triangle of distance matrix (excluding diagonal)\n",
    "            distances = distances[triu_indices(len(distances), k=1)]\n",
    "            within_class_distances.extend(distances)\n",
    "\n",
    "        # Calculate between-class distances\n",
    "        other_embeddings = embeddings[~syndrome_mask]\n",
    "        if len(syndrome_embeddings) > 0 and len(other_embeddings) > 0:\n",
    "            distances = cosine_similarity(syndrome_embeddings, other_embeddings)\n",
    "            between_class_distances.extend(distances.flatten())\n",
    "\n",
    "    embedding_stats[\"within_class_similarity\"] = {\n",
    "        \"mean\": mean(within_class_distances),\n",
    "        \"std\": std(within_class_distances),\n",
    "        \"min\": min(within_class_distances),\n",
    "        \"max\": max(within_class_distances),\n",
    "    }\n",
    "    embedding_stats[\"between_class_similarity\"] = {\n",
    "        \"mean\": mean(between_class_distances),\n",
    "        \"std\": std(between_class_distances),\n",
    "        \"min\": min(between_class_distances),\n",
    "        \"max\": max(between_class_distances),\n",
    "    }\n",
    "    return embedding_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_class_separation(embeddings: ndarray, syndrome_ids: list) -> dict:\n",
    "    \"\"\"\n",
    "    Analyze how well separated different syndrome classes are in the embedding space\n",
    "    \n",
    "    Args:\n",
    "        embeddings (np.ndarray): Matrix of embeddings\n",
    "        syndrome_ids (List): List of syndrome IDs\n",
    "        \n",
    "    Returns:\n",
    "        Dict: Statistics about class separation\n",
    "    \"\"\"\n",
    "    unique_syndromes = list(set(syndrome_ids))\n",
    "    syndrome_to_idx = {syndrome: idx for idx, syndrome in enumerate(unique_syndromes)}\n",
    "    labels = array([syndrome_to_idx[s] for s in syndrome_ids])\n",
    "    \n",
    "    inter_class_distances, intra_class_distances = [], []\n",
    "    \n",
    "    for i in range(len(unique_syndromes)):\n",
    "        class_embeddings = embeddings[labels == i]\n",
    "        \n",
    "        # intra-class distances\n",
    "        if len(class_embeddings) > 1:\n",
    "            distances = euclidean_distances(class_embeddings)\n",
    "            intra_class_distances.extend(distances[triu_indices(len(distances), k=1)])\n",
    "        \n",
    "        # inter-class distances\n",
    "        for j in range(i + 1, len(unique_syndromes)):\n",
    "            inter_class_distances.extend(\n",
    "                euclidean_distances(\n",
    "                    class_embeddings, embeddings[labels == j]\n",
    "                )\n",
    "                .flatten()\n",
    "            )\n",
    "    return {\n",
    "        'mean_inter_class_distance': mean(inter_class_distances),\n",
    "        'std_inter_class_distance': std(inter_class_distances),\n",
    "        'mean_intra_class_distance': mean(intra_class_distances),\n",
    "        'std_intra_class_distance': std(intra_class_distances),\n",
    "        'separation_ratio': mean(inter_class_distances) / mean(intra_class_distances)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_separation_metrics(embeddings: ndarray, syndrome_ids: list) -> Dict:\n",
    "    \"\"\"\n",
    "    Calculate metrics that indicate how well the embeddings separate different syndromes\n",
    "    \n",
    "    Args:\n",
    "        embeddings (np.ndarray): Matrix of embeddings\n",
    "        syndrome_ids (List): List of syndrome IDs\n",
    "\n",
    "    Returns:\n",
    "        Dict: Dictionary containing separation metrics\n",
    "    \"\"\"\n",
    "    unique_syndromes = list(set(syndrome_ids))\n",
    "    separation_metrics = {}\n",
    "\n",
    "    # Calculate syndrome centroids\n",
    "    centroids = {}\n",
    "    for syndrome in unique_syndromes:\n",
    "        syndrome_mask = array(syndrome_ids) == syndrome\n",
    "        syndrome_embeddings = embeddings[syndrome_mask]\n",
    "        centroids[syndrome] = mean(syndrome_embeddings, axis=0)\n",
    "\n",
    "    # Calculate inter-centroid distances\n",
    "    centroid_distances = {}\n",
    "    for i, syndrome1 in enumerate(unique_syndromes):\n",
    "        for syndrome2 in unique_syndromes[i + 1 :]:\n",
    "            distance = linalg.norm(centroids[syndrome1] - centroids[syndrome2])\n",
    "            centroid_distances[f\"{syndrome1}-{syndrome2}\"] = distance\n",
    "\n",
    "    separation_metrics[\"centroid_distances\"] = centroid_distances\n",
    "    separation_metrics[\"mean_centroid_distance\"] = mean(\n",
    "        list(centroid_distances.values())\n",
    "    )\n",
    "    separation_metrics[\"min_centroid_distance\"] = min(\n",
    "        list(centroid_distances.values())\n",
    "    )\n",
    "    separation_metrics[\"max_centroid_distance\"] = max(\n",
    "        list(centroid_distances.values())\n",
    "    )\n",
    "\n",
    "    return separation_metrics"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "APOLLO-wWpZYX6t",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
